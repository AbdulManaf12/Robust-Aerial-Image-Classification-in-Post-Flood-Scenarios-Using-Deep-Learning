{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_images(data_dir, img_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        \n",
    "        for image_file in tqdm(os.listdir(label_path)):\n",
    "            image_path = os.path.join(label_path, image_file)\n",
    "            \n",
    "            img = load_img(image_path, target_size=img_size)\n",
    "            img_array = img_to_array(img)\n",
    "            \n",
    "            img_array /= 255.0\n",
    "            \n",
    "            images.append(img_array)\n",
    "            labels.append(int(label)) \n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def create_data_arrays(dir, img_size, output_dir):\n",
    "    images, labels = preprocess_images(dir, img_size)\n",
    "\n",
    "    train_indices = np.arange(len(images))\n",
    "    np.random.seed(45)\n",
    "    np.random.shuffle(train_indices)\n",
    "    train_images = images[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    print(f\"Training images shape: {train_images.shape}\")\n",
    "    print(f\"Training labels shape: {train_labels.shape}\")\n",
    "\n",
    "    with open(output_dir, 'wb') as f:\n",
    "        pickle.dump((train_images, train_labels), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined dataset pickle files for training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3315/3315 [01:02<00:00, 52.91it/s] \n",
      "100%|██████████| 403/403 [00:09<00:00, 42.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (3718, 256, 256, 3)\n",
      "Training labels shape: (3718,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\A\\train\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\A_training.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:13<00:00, 51.45it/s] \n",
      "100%|██████████| 85/85 [00:02<00:00, 38.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (795, 256, 256, 3)\n",
      "Training labels shape: (795,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\A\\val\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\A_validation.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 712/712 [00:13<00:00, 52.57it/s] \n",
      "100%|██████████| 89/89 [00:02<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (801, 256, 256, 3)\n",
      "Training labels shape: (801,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\A\\test\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\A_testing.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:03<00:00, 56.32it/s] \n",
      "100%|██████████| 832/832 [00:12<00:00, 64.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (4383, 256, 256, 3)\n",
      "Training labels shape: (4383,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\B\\train\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\B_training.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [00:13<00:00, 56.37it/s] \n",
      "100%|██████████| 176/176 [00:03<00:00, 58.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (936, 256, 256, 3)\n",
      "Training labels shape: (936,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\B\\val\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\B_validation.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 764/764 [00:13<00:00, 55.62it/s] \n",
      "100%|██████████| 182/182 [00:03<00:00, 58.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (946, 256, 256, 3)\n",
      "Training labels shape: (946,)\n"
     ]
    }
   ],
   "source": [
    "input_dataset_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\B\\test\"\n",
    "pickle_folder = r\"E:\\Classification\\Focus Conference\\Datasets\\pickle_data\\B_testing.pkl\"\n",
    "img_size = (256, 256)\n",
    "create_data_arrays(input_dataset_folder, img_size, pickle_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A\n",
      "Training images for non-flood: 3315\n",
      "Validation images for non-flood: 710\n",
      "Test images for non-flood: 712\n",
      "Training images for flood: 403\n",
      "Validation images for flood: 85\n",
      "Test images for flood: 89\n"
     ]
    }
   ],
   "source": [
    "destination_directory = r\"E:\\Classification\\Focus Conference\\Datasets\\A\"\n",
    "print(\"Dataset A\")\n",
    "print(f'Training images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"train\", \"0\")))}')\n",
    "print(f'Validation images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"val\", \"0\")))}')\n",
    "print(f'Test images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"test\", \"0\")))}')\n",
    "\n",
    "print(f'Training images for flood: {len(os.listdir(os.path.join(destination_directory, \"train\", \"1\")))}')\n",
    "print(f'Validation images for flood: {len(os.listdir(os.path.join(destination_directory, \"val\", \"1\")))}')\n",
    "print(f'Test images for flood: {len(os.listdir(os.path.join(destination_directory, \"test\", \"1\")))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset B\n",
      "Training images for non-flood: 3551\n",
      "Validation images for non-flood: 760\n",
      "Test images for non-flood: 764\n",
      "Training images for flood: 832\n",
      "Validation images for flood: 176\n",
      "Test images for flood: 182\n"
     ]
    }
   ],
   "source": [
    "destination_directory = r\"E:\\Classification\\Focus Conference\\Datasets\\B\"\n",
    "print(\"Dataset B\")\n",
    "print(f'Training images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"train\", \"0\")))}')\n",
    "print(f'Validation images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"val\", \"0\")))}')\n",
    "print(f'Test images for non-flood: {len(os.listdir(os.path.join(destination_directory, \"test\", \"0\")))}')\n",
    "\n",
    "print(f'Training images for flood: {len(os.listdir(os.path.join(destination_directory, \"train\", \"1\")))}')\n",
    "print(f'Validation images for flood: {len(os.listdir(os.path.join(destination_directory, \"val\", \"1\")))}')\n",
    "print(f'Test images for flood: {len(os.listdir(os.path.join(destination_directory, \"test\", \"1\")))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
